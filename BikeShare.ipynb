{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BikeShare.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPrier/TorontoBikeShare/blob/master/BikeShare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GGr1klLRrth",
        "colab_type": "text"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp6C2RxrMQdi",
        "colab_type": "text"
      },
      "source": [
        "#### TODO\n",
        "\n",
        "\n",
        "*   Add station ids so that we can map stations later on with realtime data\n",
        "  * Need to look at realtime data to see how it is formatted\n",
        "  * Need ids so that I can properly add the feature columns for the target variable\n",
        "* Need to add target variable features -- i think 290+ columns or possibly a vector... need to look into how this can be done\n",
        "* Need to find realtime hourly weather data\n",
        "*  Create a new Dataframe that is able to hold all the needed data from both bikes and weather\n",
        "  * should be a easy method to add more data on top of\n",
        "    * match data by hour\n",
        "* Run report on new dataframe\n",
        "* Begin feature engineering \n",
        "* Build Benchmark model\n",
        "  * Look into simpler models\n",
        "* Build RNN-LTSM model\n",
        "* Look at realtime data and format training data to be similar\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTRLNjEoS_JW",
        "colab_type": "text"
      },
      "source": [
        "## NN Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RparcGZxTFIu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   Need more features, 4 is not enough\n",
        "*   Use XgBoost to improve result\n",
        "*  Get a baseline performance with a simple NN and try to improve from that \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa11xV-TS3AO",
        "colab_type": "text"
      },
      "source": [
        "## RNN Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rozFD2oTR5uD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*  For the RNN use 24 nodes, one for each hour\n",
        "* Use Bag of Words on the stations since the problem is with frequency of station usage\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRMeSoztj2BO",
        "colab_type": "text"
      },
      "source": [
        "# Google Drive Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yekiue7_n2Cw",
        "colab_type": "code",
        "outputId": "5a126201-fcb3-491b-c4f8-4221a8162e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#Authenticate and create the PyDrive client\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 993kB 593kB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7RvL1_PoBkG",
        "colab_type": "code",
        "outputId": "48322307-2321-4622-9a6a-0051c3f0dce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "folders = [\"1FywO6-NIKvfXZ3LJ08fdU_S4F6QdBOOr\",\n",
        "           \"17524iO2kiuU4_MBalRoBbYe8kpA6oVtn\"]\n",
        "\n",
        "for folderID in folders:\n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
        "      folderID)}).GetList()\n",
        "  i = 0      \n",
        "  for file1 in sorted(file_list, key = lambda x: x['title']):\n",
        "      if file1['title'].endswith(\".csv\"):\n",
        "        i+=1\n",
        "        file = drive.CreateFile({'id':file1['id']})\n",
        "        print(\"Downloading \" + str(file1['title']) + \" \" + str(i) + \"/\" + str(len(file_list)) + \" in folder \" + folderID)\n",
        "        file.GetContentFile(file1['title'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Bikeshare Ridership (2017 Q1).csv 1/6 in folder 1FywO6-NIKvfXZ3LJ08fdU_S4F6QdBOOr\n",
            "Downloading Bikeshare Ridership (2017 Q2).csv 2/6 in folder 1FywO6-NIKvfXZ3LJ08fdU_S4F6QdBOOr\n",
            "Downloading Bikeshare Ridership (2017 Q3).csv 3/6 in folder 1FywO6-NIKvfXZ3LJ08fdU_S4F6QdBOOr\n",
            "Downloading Bikeshare Ridership (2017 Q4).csv 4/6 in folder 1FywO6-NIKvfXZ3LJ08fdU_S4F6QdBOOr\n",
            "Downloading weatherstats_toronto_hourly.csv 1/1 in folder 17524iO2kiuU4_MBalRoBbYe8kpA6oVtn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPu3qNeYsvz1",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTg1x2-jxZQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pandas_profiling\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng2v3Wjkd744",
        "colab_type": "text"
      },
      "source": [
        "# Variable Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRSxtkaSbQZJ",
        "colab_type": "text"
      },
      "source": [
        "### Predictor Variables:\n",
        "* trip_start_time\n",
        "* from_station_name\n",
        "* user_type\n",
        "*  date_time_local / unixtime\n",
        "* pressure_station\n",
        "* pressure_sea\n",
        "* wind_dir_10s\n",
        "* wind_speed\n",
        "* relative_humidity\n",
        "* dew_point\n",
        "* temperature\n",
        "* windchill\n",
        "* visibility\n",
        "* health_index\n",
        "\n",
        "\n",
        "\n",
        "### Target Variable:\n",
        "  - bikes_used -- Vector for number of bikes that have left (-) or arrived (+) at each station\n",
        "\n",
        "<br />\n",
        "\n",
        "### Data_Types:\n",
        "#### Bike Share\n",
        "| Numerical             \t| Character         \t| DateTime        \t|\n",
        "|-----------------------\t|-------------------\t|-----------------\t|\n",
        "| trip_id               \t| from_station_name \t| trip_start_time \t|\n",
        "| trip_duration_seconds \t| to_station_name   \t| trip_stop_time  \t|\n",
        "| bikes_used            \t| user_type         \t|                 \t|\n",
        "\n",
        "#### Weather\n",
        "| Numerical         \t| Character \t| DateTime        \t|\n",
        "|-------------------\t|-----------\t|-----------------\t|\n",
        "| unixtime          \t| wind_dir  \t| date_time_local \t|\n",
        "| pressure_station  \t|           \t|                 \t|\n",
        "| pressure_sea      \t|           \t|                 \t|\n",
        "| wind_dir_10s      \t|           \t|                 \t|\n",
        "| wind_speed        \t|           \t|                 \t|\n",
        "| relative_humidity \t|           \t|                 \t|\n",
        "| dew_point         \t|           \t|                 \t|\n",
        "| temperature       \t|           \t|                 \t|\n",
        "| windchill         \t|           \t|                 \t|\n",
        "| visibility        \t|           \t|                 \t|\n",
        "| health_index      \t|           \t|                 \t|\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "### Variable Category\n",
        "#### Bike Share\n",
        "\n",
        "|    Categorical    | Continuous            |\n",
        "|:-----------------:|-----------------------|\n",
        "| trip_id           | trip_start_time       |\n",
        "| from_station_name | trip_stop_time        |\n",
        "| to_station_name   | trip_duration_seconds |\n",
        "| user_type         | bikes_used            |\n",
        "\n",
        "#### Weather\n",
        "\n",
        "| Categorical \t| Continuous        \t|\n",
        "|-------------\t|-------------------\t|\n",
        "| wind_dir    \t| date_time_local   \t|\n",
        "|             \t| unixtime          \t|\n",
        "|             \t| pressure_station  \t|\n",
        "|             \t| pressure_sea      \t|\n",
        "|             \t| wind_dir_10s      \t|\n",
        "|             \t| wind_speed        \t|\n",
        "|             \t| relative_humidity \t|\n",
        "|             \t| dew_point         \t|\n",
        "|             \t| temperature       \t|\n",
        "|             \t| windchill         \t|\n",
        "|             \t| visibility        \t|\n",
        "|             \t| health_index      \t|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf7ak-POx6u5",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9dca3e9x24O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Issues with the BikeShare Dataset:\n",
        "  - Q1 and Q2 are different from Q3 and Q4 in these ways:\n",
        "      - station ids are non existent in only Q3 and Q4\n",
        "      - date format switches from d-m-y to m-d-y for Q3 to a different one for Q4\n",
        "      - Q4 uses a datetime var? excel shows a different format than what is actually there\n",
        "'''\n",
        "bs_files = [\"Bikeshare Ridership (2017 Q1).csv\", \n",
        "      \"Bikeshare Ridership (2017 Q2).csv\", \n",
        "      \"Bikeshare Ridership (2017 Q3).csv\",\n",
        "      \"Bikeshare Ridership (2017 Q4).csv\"]\n",
        "\n",
        "QX = {0:bs_files[0], 1:bs_files[:2], 2:bs_files[:3], 3:bs_files}\n",
        "\n",
        "def read_bikeshare_data(quarters):\n",
        "  files = QX[quarters]\n",
        "  df = pd.read_csv(files[0]).dropna()\n",
        "  \n",
        "  df = format_time(0, df)\n",
        "  \n",
        "  for i in range(len(files)-1):\n",
        "    temp_df = pd.read_csv(files[i+1]).dropna()\n",
        "    temp_df = format_time(i+1, temp_df)\n",
        "    \n",
        "    df = df.append(temp_df, sort=False)\n",
        "  \n",
        "  # Remove station ids as Q3 and Q4 do not have them (still have station names)\n",
        "  if 'from_station_id' in df.columns:\n",
        "      df.drop(['from_station_id', 'to_station_id'], axis=1, inplace=True)\n",
        "  \n",
        "  df[\"from_station_name\"] = df[\"from_station_name\"].str.replace(\".\", \"\")\n",
        "  df[\"from_station_name\"] = df[\"from_station_name\"].str.replace(\"'\", \"\")\n",
        "  df[\"to_station_name\"] = df[\"to_station_name\"].str.replace(\".\", \"\")\n",
        "  df[\"to_station_name\"] = df[\"to_station_name\"].str.replace(\"'\", \"\")\n",
        "  \n",
        "  return df\n",
        "\n",
        "def format_time(quarter, df):\n",
        "  if quarter == 0 or quarter == 1:\n",
        "    df['trip_start_time'] = pd.to_datetime(df['trip_start_time'], \n",
        "                                                    format='%d/%m/%Y %H:%M')\n",
        "    df['trip_stop_time'] = pd.to_datetime(df['trip_stop_time'], \n",
        "                                                   format='%d/%m/%Y %H:%M')\n",
        "  elif quarter == 3:\n",
        "    df['trip_start_time'] = pd.to_datetime(df['trip_start_time'], \n",
        "                                                    format='%m/%d/%y %H:%M:%S')\n",
        "    df['trip_stop_time'] = pd.to_datetime(df['trip_stop_time'], \n",
        "                                                   format='%m/%d/%y %H:%M:%S')\n",
        "  else:\n",
        "    df['trip_start_time'] = pd.to_datetime(df['trip_start_time'], \n",
        "                                                    format='%m/%d/%Y %H:%M')\n",
        "    df['trip_stop_time'] = pd.to_datetime(df['trip_stop_time'], \n",
        "                                                    format='%m/%d/%Y %H:%M')\n",
        "  return df\n",
        "\n",
        "def read_weather_data(quarters):\n",
        "  df = pd.read_csv('weatherstats_toronto_hourly.csv')\n",
        "  df = df[df['unixtime'].between(1483228800, 1514779200, inclusive=True)]\n",
        "  return df\n",
        "\n",
        "def read_data(quarters):\n",
        "  df_bikes = read_bikeshare_data(quarters)\n",
        "  df_weather = read_weather_data(quarters)\n",
        "  \n",
        "  # TODO:  Create new column that has date and hour in each df\n",
        "  df_bikes['hour'] = df_bikes['trip_start_time'].dt.hour\n",
        "  df_weather['hour'] = df_weather['date_time_local'].dt.hour\n",
        "  # TODO:  Merge the dfs to another df\n",
        "  # TODO:  Drop the new column and return the new df\n",
        "  return df_bikes, df_weather"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeMu935K0FbR",
        "colab_type": "text"
      },
      "source": [
        "# Run EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iac1Ozf70J7G",
        "colab_type": "code",
        "outputId": "2f30cf7b-7c08-4a34-9629-79d473bef71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "'''\n",
        "-----TODO-----\n",
        "- Read in and manipulate/clean data\n",
        "- visualize data \n",
        "- perform more EDA\n",
        "- predict rider usage on a day of the year?? (would need to add in another dataset -- possibly weather, holidays and events)\n",
        "'''\n",
        "# quarters to read in (just for speed of debugging)\n",
        "quarters = 3\n",
        "df_bikes, df_weather = read_data(quarters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-641eb198aa66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# quarters to read in (just for speed of debugging)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mquarters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf_bikes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquarters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-03c6ca758795>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(quarters)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;31m# TODO:  Create new column that has date and hour in each df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0mdf_bikes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_bikes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trip_start_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   \u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_weather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date_time_local'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;31m# TODO:  Merge the dfs to another df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;31m# TODO:  Drop the new column and return the new df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5061\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[1;32m   5062\u001b[0m                 name in self._accessors):\n\u001b[0;32m-> 5063\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mpass\u001b[0m  \u001b[0;31m# we raise an attribute error anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         raise AttributeError(\"Can only use .dt accessor with datetimelike \"\n\u001b[0m\u001b[1;32m    325\u001b[0m                              \"values\")\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh73Wmxm3hoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report = pandas_profiling.ProfileReport(df_bikes)\n",
        "report.to_file(outputfile='bikesReport.html')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1deUIcq5sqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report = pandas_profiling.ProfileReport(df_weather)\n",
        "report.to_file(outputfile='weatherReport.html')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyHfkCpbKpGW",
        "colab_type": "text"
      },
      "source": [
        "## Feature manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA4-JlrPKt08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove bad features according to the pandas report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptr39NMe2dX-",
        "colab_type": "text"
      },
      "source": [
        "# Benchmark Model\n",
        "\n",
        "A basic NN built as a benchmark to compare the later models to, to see if the more complex models are making a difference compared to a simplier model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i71ViXSm8CS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: go from pandas df to sklearn training/Testing set"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}